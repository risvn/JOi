#!/bin/bash

# ---------------- Config ----------------
laptop_ip="192.168.231.189"
port=8080
llama_url="http://$laptop_ip:$port/completion"

whisper="../stt/bin/whisper-cli"
txt_model="../stt/models/ggml-tiny.bin"
audio_file="../audio/speech.wav"

piper="../tts/piper/piper"
tts_model="../tts/voice/libritts_r/en_US-libritts_r-medium.onnx"
beep_sound="../audio/beep/bing.mp3"

echo "üéôÔ∏è Pill Assistant (Client Mode - using laptop's LLaMA) Started"

# ---------------- Main Loop ----------------
while true; do
  echo ""
  mpg123 -q "$beep_sound"
  echo "[üî¥ Listening ...]"

  # 1.  Beep before recording
  # 2. Record audio
  arecord -q -f S16_LE -r 16000 -c 1 -d 5 "$audio_file"


  echo "[ Transcribing speech to text...]"
  "$whisper" -m "$txt_model" -f "$audio_file" -otxt > /dev/null 2>&1
  transcript=$(cat "${audio_file}.txt")

  if [ -z "$transcript" ]; then
    echo "‚ö†Ô∏èDidn't catch that. Please speak clearly."
    continue
  fi

  echo "[< You said: \"$transcript\">]"

  echo "[Query processing...]"
  # 4. Generate full LLM prompt using query_handler.py
  prompt=$(python3 ./query_handler.py "$transcript")
  

  echo "$prompt"

  # 5. Send prompt to LLaMA server
  echo "[generating respons...]"
  response=$(curl -s "$llama_url" \
    -X POST \
    -H "Content-Type: application/json" \
    -d "$(jq -nc --arg prompt "$prompt" '{"prompt": $prompt, "n_predict": 128}')" \
    | jq -r '.content')

  echo " < Response >: $response"

  # 6. Speak it out
  echo "[üîä Speaking...]"
  echo "$response" | "$piper" --model "$tts_model" --output-raw 2>/dev/null | aplay -q -f S16_LE -r 22050

  echo "----- Ready for the next query.------"
done
